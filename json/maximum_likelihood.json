{"cursor":"48230","size":15,"audio":[],"currentlang":"en","article":"\n\nIn statistics, 'maximum-likelihood estimation' ('MLE') is a method of estimating\nthe parameters of a statistical model. When applied to a data set and given a\nstatistical model, maximum-likelihood estimation provides estimates for the\nmodel's parameters.\n\nThe method of maximum likelihood corresponds to many well-known estimation\nmethods in statistics. For example, one may be interested in the heights of\nadult female penguins, but be unable to measure the height of every single\npenguin in a population due to cost or time constraints. Assuming that the\nheights are normally (Gaussian) distributed with some unknown mean and variance,\nthe mean and variance can be estimated with MLE while only knowing the heights\nof some sample of the overall population. MLE would accomplish this by taking\nthe mean and variance as parameters and finding particular parametric values\nthat make the observed results the most probable (given the model).\n\nIn general, for a fixed set of data and underlying statistical model, the\nmethod of maximum likelihood selects the set of values of the model parameters\nthat maximizes the likelihood function. Intuitively, this maximizes the\n\"agreement\" of the selected model with the observed data, and for discrete\nrandom variables it indeed maximizes the probability of the observed data under\nthe resulting distribution. Maximum-likelihood estimation gives a unified\napproach to estimation, which is well-defined in the case of the normal\ndistribution and many other problems. However, in some complicated problems,\ndifficulties do occur: in such problems, maximum-likelihood estimators are\nunsuitable or do not exist.\n","linknr":-1,"url":"maximum_likelihood","recorded":1376429068,"links":[],"instances":["statistics","mathematics","continuous","mathematics","statistics","statistics"],"pdf":[],"categories":["Estimation theory","Statistical theory","M-estimators","Fitting probability distributions"],"headings":["Principles","Properties","Examples","Applications","History","See also","References","Further reading","External links"],"image":[],"tags":[["optimization","mathematics"],["homogeneity","statistics"],["uniform_distribution","continuous"],["asymptotic_theory","statistics"],["uniform_distribution","continuous"],["range","mathematics"],["method_of_moments","statistics"]],"members":["method_of_moments","optimization","uniform_distribution","range","asymptotic_theory","homogeneity"],"related":["Estimator","Parameter","Statistical_model","Statistical_model","Estimator","Normal_distribution","Mean","Variance","Likelihood_function","Well_defined","Normal_distribution","Independent_and_identically_distributed","Probability_density_function","Parametric_model","Independent_and_identically_distributed","Likelihood","Circumflex","Monotonic_function","Optimization_(mathematics)","Iterative_methods","Supremum","Independent_and_identically_distributed","Homogeneity_(statistics)","Time_series","Maximum_a_posteriori","Bayesian_estimator","Uniform_distribution_(continuous)","Prior_probability","Parameter_space","Extremum_estimator","Asymptotic_theory_(statistics)","Consistency_of_an_estimator","Asymptotic_normality","Fisher_information","Efficient_estimator","Cramér–Rao_lower_bound","Mean_squared_error","Convergence_in_probability","Almost_sure_convergence","Compact_set","Concave_function","Level_set","Upper_semi-continuous","Uniform_law_of_large_numbers","I.i.d.","Stochastic_equicontinuity","Almost_sure_convergence","Uniform_distribution_(continuous)","Compact_space","Bias_of_an_estimator","Nuisance_parameter","Independent_identically_distributed","Consistent_estimator","Taylor_series","Law_of_large_numbers","Continuous_mapping_theorem","Central_limit_theorem","Fisher_information","Slutsky's_theorem","Cramér–Rao_bound","Fisher_information_matrix","Bias_of_an_estimator","Edgeworth_expansion","Einstein_notation","Expected_value","Unfair_coin","Probability_mass_function","Binomial_distribution","Derivative","Bernoulli_trial","Normal_distribution","Probability_density_function","Probability_density_function","Independent_identically_distributed","Sample_mean","Natural_logarithm","Continuous_function","Strictly_increasing","Range_(mathematics)","Information_entropy","Fisher_information","Expectation_value","Statistical_error","Covariance_matrix","Expectation-maximization_algorithm","Kalman_filter","Linear_model","Generalized_linear_model","Factor_analysis","Confirmatory_factor_analysis","Structural_equation_modeling","Hypothesis_testing","Confidence_interval","Discrete_choice","Communication_systems","Psychometrics","Econometrics","Computational_phylogenetics","Mathematical_proof","Ronald_Fisher","Gauss","Laplace","Thorvald_N._Thiele","Francis_Ysidro_Edgeworth","Bayesian_statistics","Generalized_method_of_moments","M-estimator","Maximum_a_posteriori","Maximum_spacing_estimation","Method_of_moments_(statistics)","Method_of_support","Minimum_distance_estimation","Quasi-maximum_likelihood","Restricted_maximum_likelihood","BHHH_algorithm","Extremum_estimator","Fisher_information","Likelihood_function","Mean_squared_error","Rao–Blackwell_theorem","Mean_squared_error","Sufficient_statistic","Debabrata_Basu","Journal_of_Mathematical_Psychology"]}